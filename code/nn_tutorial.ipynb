{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch.nn\n",
    "by Jeremy Howard, `fast.ai <https://www.fast.ai>`    \n",
    "\n",
    "硬件：Station  \n",
    "软件：fusimeng/ai.pytorch:v5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch提供了torch.nn、torch.optim、Dataset和DataLoader这些设计优雅的模块和类以帮助使用者创建和训练神经网络。 为了最大化利用这些模块和类的功能，并使用它们做出适用于你所研究问题的模型，你需要真正理解他们是如何工作的。 为了做到这一点，我们首先基于MNIST数据集训练一个没有任何特征的简单神经网络。 最开始我们只会用到PyTorch中最基本的tensor功能，然后我们将会逐渐的从torch.nn，torch.optim，Dataset，DataLoader中选择一个特征加入到模型中，来展示新加入的特征会对模型产生什么样的效果，以及它是如何使模型变得更简洁或更灵活。\n",
    "\n",
    "## 一、MNIST data setup\n",
    "\n",
    "我们将使[pathlib](https://docs.python.org/zh-cn/3/library/pathlib.html) 来处理路径（Python 3标准库的一部分），并将使用请求下载数据集 。我们只会在使用模块时导入模块，因此您可以准确地看到每个模块的使用情况。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"Data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集采用numpy数组格式，并使用pickle存储，pickle是一种特定于python的格式，用于序列化数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个图像为28 x 28，并存储为长度为784（= 28x28）的扁平行。我们来看一个; 我们需要先将其重塑为2d。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用torch.tensor而不是numpy数组，因此我们需要转换数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、从头开始的神经网络（没有torch.nn）\n",
    "让我们首先使用PyTorch张量操作创建一个模型。我们假设您已经熟悉神经网络的基础知识。（如果你不是，你可以在[course.fast.ai](https://course.fast.ai/)学习它们）。\n",
    "\n",
    "PyTorch提供了创建随机或零填充张量的方法，我们将使用这些方法为简单的线性模型创建权重和偏差。这些只是常规张量，有一个非常特别的补充：我们告诉PyTorch它们需要一个渐变。这会导致PyTorch记录在张量上完成的所有操作，以便它可以在反向传播过程中自动计算梯度！\n",
    "\n",
    "对于权重，我们requires_grad 在初始化之后设置，因为我们不希望梯度中包含该步骤。（请注意，PyTorch _中的跟踪表示操作是就地执行的。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于PyTorch能够自动计算渐变，我们可以使用任何标准的Python函数（或可调用对象）作为模型！因此，让我们编写一个简单的矩阵乘法和广播加法来创建一个简单的线性模型。我们还需要一个激活函数，所以我们将编写log_softmax并使用它。记住：虽然PyTorch提供了许多预先编写的丢失函数，激活函数等，但您可以使用普通的python轻松编写自己的函数。PyTorch甚至可以自动为您的功能创建快速GPU或矢量化CPU代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面，@代表点积运算。我们将在一批数据上调用我们的函数（在这种情况下，64个图像）。这是一个前进传球。请注意，我们的预测在这个阶段不会比随机更好，因为我们从随机权重开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.6939, -2.7041, -1.8446, -2.8252, -2.3655, -2.1982, -2.9234, -2.0235,\n",
      "        -2.4049, -1.7901], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如您所见，preds张量不仅包含张量值，还包含渐变函数。我们稍后会用它来做backprop。\n",
    "\n",
    "让我们实现负对数似然用作损失函数（同样，我们可以使用标准Python）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用我们的随机模型检查我们的损失，这样我们就可以看到我们是否在后面的backprop传递后改进了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4342, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还实现了一个计算模型精度的函数。对于每个预测，如果具有最大值的索引与目标值匹配，则预测是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查一下随机模型的准确性，这样我们就可以看出随着损失的改善我们的准确度是否会提高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1250)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以运行一个训练循环。对于每次迭代，我们将：\n",
    "\n",
    "* 选择一小批数据（大小bs）\n",
    "* 使用该模型进行预测\n",
    "* 计算损失\n",
    "* loss.backward()在这种情况下，更新模型的渐变，weights 和bias。\n",
    "  \n",
    "我们现在使用这些渐变来更新权重和偏差。我们在torch.no_grad()上下文管理器中执行此操作，因为我们不希望记录这些操作以用于下一次计算渐变。您可以在此处详细了解PyTorch的Autograd如何记录操作 。\n",
    "\n",
    "然后我们将梯度设置为零，以便我们为下一个循环做好准备。否则，我们的渐变将记录已发生的所有操作的运行记录（即将渐变loss.backward() 添加到已存储的任何内容，而不是替换它们）。   \n",
    "小贴士\n",
    "\n",
    "您可以使用标准python调试器对PyTorch代码进行单步调试，从而在每一步检查不同的变量值。取消下面的set_trace()来尝试该功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这样：我们创建并训练了一个最小的神经网络（在这种情况下，逻辑回归，因为我们没有隐藏的层）完全从头开始！\n",
    "\n",
    "让我们检查损失和准确性，并将它们与我们之前得到的结果进行比较。我们预计损失会减少，准确性会增加，而且他们有。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0805, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、使用torch.nn.functional\n",
    "我们现在将重构我们的代码，以便它像以前一样做，只有我们才会开始利用PyTorch的nn类来使它更简洁和灵活。在这里的每一步，我们应该使我们的代码中的一个或多个：更短，更易理解和/或更灵活。\n",
    "\n",
    "第一个也是最简单的步骤是通过将手写的激活和丢失函数替换为来自torch.nn.functional （通常F按惯例导入到命名空间中）的函数来缩短代码。该模块包含torch.nn库中的所有函数（而库的其他部分包含类）。除了各种各样的丢失和激活功能外，您还可以在这里找到一些用于创建神经网络的便捷功能，例如池功能。（还有用于进行卷积，线性层等的函数，但正如我们将看到的，通常可以使用库的其他部分更好地处理这些函数。）\n",
    "\n",
    "如果您使用负对数似然丢失和记录softmax激活，那么Pytorch提供了一个F.cross_entropy将两者结合起来的功能。所以我们甚至可以从我们的模型中删除激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，我们不再调用log_softmax该model函数。让我们确认我们的损失和准确性与以前相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0805, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、使用nn.Module重构\n",
    "接下来，我们将使用nn.Module和nn.Parameter，以获得更清晰，更简洁的训练循环。我们子类nn.Module（它本身是一个类，能够跟踪状态）。在这种情况下，我们想要创建一个包含前向步骤的权重，偏差和方法的类。 我们将使用nn.Module许多属性和方法（例如.parameters()和.zero_grad()）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们现在使用的是对象而不是仅使用函数，因此我们首先必须实例化我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以像以前一样计算损失。请注意， nn.Module对象被用作函数（即它们是 可调用的），但在幕后，Pytorch将forward 自动调用我们的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4095, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前，对于我们的训练循环，我们必须按名称更新每个参数的值，并分别手动将每个参数的梯度归零，如下所示：\n",
    "```\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()\n",
    "```\n",
    "现在我们可以利用model.parameters（）和model.zero_grad（）（它们都由PyTorch定义nn.Module）来使这些步骤更简洁，更不容易忘记我们的一些参数，特别是如果我们有一个更复杂的模型：\n",
    "```\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "我们将把我们的小训练循环包装在一个fit函数中，以便稍后再运行它。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们仔细检查一下我们的损失是否已经下降："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0822, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、使用nn.Linear重构\n",
    "我们继续重构我们的代码。我们将使用Pytorch类nn.Linear而不是手动定义和初始化self.weights和self.bias计算，而是使用 线性层，它为我们做了所有这些。Pytorch有许多类型的预定义层，可以大大简化我们的代码，并且通常也使它更快。xb  @ self.weights + self.bias\n",
    "```python\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "```\n",
    "我们实例化我们的模型并以与以前相同的方式计算损失：\n",
    "```\n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our model and calculate the loss in the same way as before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2493, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still able to use our same ``fit`` method as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0820, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六、重构使用optim\n",
    "Pytorch还有一个包含各种优化算法的包torch.optim。我们可以使用step优化器中的方法来执行前进步骤，而不是手动更新每个参数。\n",
    "\n",
    "这将让我们取代之前的手动编码优化步骤：\n",
    "```\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "而只是使用：\n",
    "```\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```\n",
    "（optim.zero_grad()将渐变重置为0，我们需要在计算下一个小批量的渐变之前调用它。）\n",
    "\n",
    "from torch import optim\n",
    "我们将定义一个小函数来创建我们的模型和优化器，以便将来可以重用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a little function to create our model and optimizer so we\n",
    "can reuse it in the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、使用数据集重构\n",
    "PyTorch有一个抽象的Dataset类。数据集可以是任何具有__len__函数（由Python的标准len函数调用）和__getitem__函数作为索引的方式。 本教程 将介绍创建自定义FacialLandmarkDataset类作为子类的一个很好的示例Dataset。\n",
    "\n",
    "PyTorch的TensorDataset 是一个包含张量的数据集。通过定义索引的长度和方式，这也为我们提供了一种沿张量的第一维迭代，索引和切片的方法。这将使我们更容易在我们训练的同一行中访问独立变量和因变量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "二者x_train并y_train可以在一个单一的组合TensorDataset，这将是容易遍历和切片。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "以前，我们必须分别迭代x和y值的小批量：\n",
    "\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "现在，我们可以一起完成这两个步骤：\n",
    "\n",
    "xb,yb = train_ds[i*bs : i*bs+bs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0813, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八、使用DataLoader重构\n",
    "Pytorch DataLoader负责管理批次。您可以创建一个DataLoader从任何Dataset。DataLoader使批量迭代变得更容易。DataLoader 不是必须使用，而是自动为我们提供每个小批量。train_ds[i*bs : i*bs+bs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前，我们的循环遍历批次（xb，yb），如下所示：\n",
    "```\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "```\n",
    "现在，我们的循环更清晰，因为（xb，yb）是从数据加载器自动加载的：\n",
    "```\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0827, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于Pytorch的nn.Module，nn.Parameter，Dataset，和DataLoader，我们的训练循环现显着更小，更容易理解。现在让我们尝试添加在实践中创建有效模型所需的基本功能。\n",
    "\n",
    "## 九、添加验证\n",
    "在第1部分中，我们只是尝试设置合理的训练循环以用于我们的训练数据。实际上，您总是应该有一个验证集，以确定您是否过度拟合。\n",
    "\n",
    "改变训练数据 对于防止批次与过度拟合之间的相关性非常 重要。另一方面，无论我们是否对验证集进行洗牌，验证损失都是相同的。由于改组需要额外的时间，因此对验证数据进行洗牌是没有意义的。\n",
    "\n",
    "我们将使用批量大小作为验证集，其大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用更少的内存（它不需要存储渐变）。我们利用这一点来使用更大的批量大小并更快地计算损失。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在每个时代结束时计算并打印验证损失。\n",
    "\n",
    "（请注意，我们总是model.train()在训练之前和model.eval() 推理之前调用，因为这些是由层使用的，例如nn.BatchNorm2d 并nn.Dropout确保这些不同阶段的适当行为。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3207)\n",
      "1 tensor(0.2862)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十、创建fit（）和get_data（）\n",
    "我们现在将对自己进行一些重构。由于我们经历了两次计算训练集和验证集损失的类似过程，因此我们将其转换为自己的函数loss_batch，该函数计算一个批次的损失。\n",
    "\n",
    "我们为训练集传递一个优化器，并使用它来执行backprop。对于验证集，我们不传递优化器，因此该方法不执行backprop。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit 运行必要的操作来训练我们的模型并计算每个时期的训练和验证损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_data`` returns dataloaders for the training and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the\n",
    "model can be run in 3 lines of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4850252965450287\n",
      "1 0.30622428317070005\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these basic 3 lines of code to train a wide variety of models.\n",
    "Let's see if we can use them to train a convolutional neural network (CNN)!\n",
    "\n",
    "## 十一、Switch to CNN\n",
    "\n",
    "我们现在要用三个卷积层构建我们的神经网络。因为上一节中没有任何函数假定模型形式的任何内容，我们将能够使用它们来训练CNN而无需任何修改。\n",
    "\n",
    "我们将使用Pytorch的预定义 Conv2d类作为卷积层。我们定义了一个带有3个卷积层的CNN。每个卷积后面都有一个ReLU。最后，我们执行平均合并。（注意这view是PyTorch的numpy版本 reshape）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation on\n",
    "stochastic gradient descent that takes previous updates into account as well\n",
    "and generally leads to faster training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.44883635759353635\n",
      "1 0.2483194197177887\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十二、nn.Sequential\n",
    "torch.nn有另一个方便的类我们可以使用简单的代码： 顺序。甲Sequential对象运行的每个包含在其内的模块的，以顺序的方式。这是编写神经网络的一种更简单的方法。\n",
    "\n",
    "要利用这一点，我们需要能够轻松地从给定函数定义 自定义图层。例如，PyTorch没有视图层，我们需要为我们的网络创建一个。Lambda 将创建一个层，我们可以在定义网络时使用 Sequential。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model created with ``Sequential`` is simply:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4737539008140564\n",
      "1 0.2766986144065857\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十三、包装DataLoader\n",
    "我们的CNN相当简洁，但它只适用于MNIST，因为：\n",
    "它假设输入是28 * 28长矢量\n",
    "它假设最终CNN网格大小为4 * 4（因为这是平均值）\n",
    "汇集我们使用的内核大小）\n",
    "\n",
    "让我们摆脱这两个假设，因此我们的模型适用于任何二维单通道图像。首先，我们可以删除初始Lambda层，但将数据预处理移动到生成器中：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n",
    "allows us to define the size of the *output* tensor we want, rather than\n",
    "the *input* tensor we have. As a result, our model will work with any\n",
    "size input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.33956847071647644\n",
      "1 0.2703994888305664\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "十四、Using your GPU\n",
    "---------------\n",
    "\n",
    "If you're lucky enough to have access to a CUDA-capable GPU (you can\n",
    "rent one for about $0.50/hour from most cloud providers) you can\n",
    "use it to speed up your code. First check that your GPU is working in\n",
    "Pytorch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create a device object for it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update ``preprocess`` to move batches to the GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can move our model to the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find it runs faster now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1940934679031372\n",
      "1 0.1676538721561432\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十六、结束思想\n",
    "我们现在有一个通用数据管道和训练循环，您可以使用它来使用Pytorch训练多种类型的模型。要了解模型现在的简单培训，请查看mnist_sample示例笔记本。\n",
    "\n",
    "当然，您需要添加许多内容，例如数据增强，超参数调整，监控培训，转移学习等等。这些功能在fastai库中提供，该库使用本教程中显示的相同设计方法开发，为希望进一步采用其模型的从业者提供了自然的下一步。\n",
    "\n",
    "我们承诺在本教程的开始，我们会通过例如每个解释 torch.nn，torch.optim，Dataset，和DataLoader。那么让我们总结一下我们所看到的：\n",
    "\n",
    "* torch.nn\n",
    "    * Module：创建一个callable，其行为类似于函数，但也可以包含状态（例如神经网络层权重）。它知道Parameter它包含什么，并且可以将所有渐变归零，循环通过它们以进行重量更新等。\n",
    "    * Parameter：张量的包装器，告诉Module它在backprop期间它有需要更新的权重。仅更新具有requires_grad属性集的张量\n",
    "    * functional：一个模块（通常F按惯例导入到命名空间中），它包含激活函数，损失函数等，以及层的非有状态版本，如卷积层和线性层。\n",
    "* torch.optim：包含优化程序，例如SGD，更新Parameter后退步骤中的权重\n",
    "* Dataset：带有a __len__和a 的对象的抽象接口__getitem__，包括用Pytorch提供的类，如TensorDataset\n",
    "* DataLoader：获取任何Dataset并创建一个返回批量数据的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
