# åˆ†å¸ƒå¼é€šä¿¡åŒ… - torch.distributed
* V1.2    

* å‚è€ƒ[ğŸ”—](https://pytorch.apachecn.org/docs/1.2/distributed.html#)

## ä¸€ã€åç«¯
![](../imgs/api/01.png)    
* MPI åŸºç¡€æ¦‚å¿µï¼š[ğŸ”—](https://github.com/fusimeng/ParallelComputing/blob/master/notes/mpiconcept.md)   
* MPIé€šä¿¡[ğŸ”—](https://github.com/fusimeng/ParallelComputing/blob/master/notes/communication.md)/[ğŸ”—](https://github.com/fusimeng/ParallelComputing/blob/master/notes/CollectiveCommunication.md)   

## äºŒã€PyTorché™„å¸¦çš„åç«¯
åç«¯é€‰æ‹©åŸåˆ™
## ä¸‰ã€å¸¸è§çš„ç¯å¢ƒå˜é‡

## å››ã€åŸºæœ¬
å¯¹æ¯”
* torch.nn.parallel.DistributedDataParallel()
* torch.multiprocessing 
* torch.nn.DataParallel() 

## äº”ã€åˆå§‹åŒ–
torch.distributed.init_process_group(backend, init_method='env://', timeout=datetime.timedelta(seconds=1800), **kwargs)

å‚æ•°:

* backend (str or Backend) â€“ åç«¯ä½¿ç”¨ã€‚æ ¹æ®æ„å»ºæ—¶é…ç½®ï¼Œæœ‰æ•ˆå€¼åŒ…æ‹¬ mpiï¼Œglooå’Œncclã€‚è¯¥å­—æ®µåº”è¯¥ä»¥å°å†™å­—ç¬¦ä¸²å½¢å¼ç»™å‡º(ä¾‹å¦‚"gloo")ï¼Œä¹Ÿå¯ä»¥é€šè¿‡Backendè®¿é—®å±æ€§(ä¾‹å¦‚Backend.GLOO)ã€‚
* init_method (str, optional) â€“ æŒ‡å®šå¦‚ä½•åˆå§‹åŒ–è¿›ç¨‹ç»„çš„URLã€‚
* world_size (int, optional) â€“ å‚ä¸ä½œä¸šçš„è¿›ç¨‹æ•°ã€‚
* rank (int, optional) â€“ å½“å‰æµç¨‹çš„æ’åã€‚
* timeout (timedelta__, optional) â€“ é’ˆå¯¹è¿›ç¨‹ç»„æ‰§è¡Œçš„æ“ä½œè¶…æ—¶ï¼Œé»˜è®¤å€¼ç­‰äº30åˆ†é’Ÿï¼Œè¿™ä»…é€‚ç”¨äºglooåç«¯ã€‚
* group_name (str, optional__, deprecated) â€“ å›¢é˜Ÿåå­—ã€‚

ç›®å‰æ”¯æŒä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ï¼š

### 1ã€TCPåˆå§‹åŒ–
### 2ã€å…±äº«æ–‡ä»¶ç³»ç»Ÿåˆå§‹åŒ–
### 3ã€ç¯å¢ƒå˜é‡åˆå§‹åŒ–



